{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "length=1648\n",
    "data_path='.../data2.xlsx'\n",
    "save_path='...'\n",
    "data_path_add='.../4d_drills.xlsx'\n",
    "save_path_add='...'\n",
    "distance_path='.../distance.csv'\n",
    "plt.rcParams['axes.unicode_minus']=False\n",
    "plt.rcParams['font.size'] = 15\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "font={'family' : 'Arial',\n",
    "        'color'  : 'black',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 15,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creater data.npz\n",
    "All_data=np.asarray(pd.read_excel(data_path,sheet_name='Sheet1'))\n",
    "soil=All_data[0:length,4].astype('float') \n",
    "w=All_data[0:length,5].astype('float') \n",
    "rou=All_data[0:length,6].astype('float') \n",
    "e=All_data[0:length,7].astype('float') \n",
    "Sr=All_data[0:length,8].astype('float') \n",
    "cord=All_data[0:length,9].astype('float') \n",
    "Es=All_data[0:length,10].astype('float') \n",
    "out_data=np.zeros((length,1,6))\n",
    "out_data[:,0,0]=Es\n",
    "out_data[:,0,1]=soil\n",
    "out_data[:,0,2]=w\n",
    "out_data[:,0,3]=e\n",
    "out_data[:,0,4]=Sr\n",
    "out_data[:,0,5]=rou\n",
    "data_path_npz=save_path+'/data.npz'\n",
    "np.savez(data_path_npz,data=out_data)\n",
    "\n",
    "out_data_2=np.zeros((length,1,6))\n",
    "out_data_2[:,0,0]=Es\n",
    "out_data_2[:,0,1]=soil\n",
    "out_data_2[:,0,2]=w\n",
    "out_data_2[:,0,3]=e\n",
    "out_data_2[:,0,4]=Sr\n",
    "out_data_2[:,0,5]=cord\n",
    "data_path_npz_2=save_path+'/data2.npz'\n",
    "#np.savez(data_path_npz_2,data=out_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_c, hid_c, out_c):\n",
    "        \"\"\"\n",
    "        GCN\n",
    "        :param in_c: input channels\n",
    "        :param hid_c:  hidden nodes\n",
    "        :param out_c:  output channels\n",
    "        \"\"\"\n",
    "        super(GCN, self).__init__()\n",
    "        self.linear_1 = nn.Linear(in_c, hid_c)\n",
    "        self.linear_2 = nn.Linear(hid_c, out_c)\n",
    "        self.act = nn.ReLU()\n",
    "    def forward(self, data):\n",
    "        graph_data = data[\"graph\"][0]  # [N, N]\n",
    "        graph_data = self.process_graph(graph_data)\n",
    "        flow_x = data[\"flow_x\"]  # [B, N, H, D]\n",
    "        B, N = flow_x.size(0), flow_x.size(1)\n",
    "        flow_x = flow_x.view(B, N, -1)  # [B, N, H*D]  H = 6, D = 1\n",
    "        output_1 = self.linear_1(flow_x)  # [B, N, hid_C]\n",
    "        output_1 = self.act(torch.matmul(graph_data, output_1))  # [N, N], [B, N, Hid_C]\n",
    "        output_2 = self.linear_2(output_1)\n",
    "        output_2 = self.act(torch.matmul(graph_data, output_2))  # [B, N, 1, Out_C]\n",
    "        return output_2.unsqueeze(2)\n",
    "    @staticmethod\n",
    "    def process_graph(graph_data):\n",
    "        N = graph_data.size(0)\n",
    "        matrix_i = torch.eye(N, dtype=graph_data.dtype, device=graph_data.device)\n",
    "        graph_data += matrix_i  # A~ [N, N]\n",
    "        degree_matrix = torch.sum(graph_data, dim=-1, keepdim=False)  # [N]\n",
    "        degree_matrix = degree_matrix.pow(-1)\n",
    "        degree_matrix[degree_matrix == float(\"inf\")] = 0.  # [N]\n",
    "        degree_matrix = torch.diag(degree_matrix)  # [N, N]\n",
    "        return torch.mm(degree_matrix, graph_data)  # D^(-1) * A = \\hat(A)\n",
    "class ChebConv(nn.Module):\n",
    "    def __init__(self, in_c, out_c, K, bias=True, normalize=True):\n",
    "        \"\"\"\n",
    "        ChebNet conv\n",
    "        :param in_c: input channels\n",
    "        :param out_c:  output channels\n",
    "        :param K: the order of Chebyshev Polynomial\n",
    "        :param bias:  if use bias\n",
    "        :param normalize:  if use norm\n",
    "        \"\"\"\n",
    "        super(ChebConv, self).__init__()\n",
    "        self.normalize = normalize\n",
    "        self.weight = nn.Parameter(torch.Tensor(K + 1, 1, in_c, out_c))  # [K+1, 1, in_c, out_c]\n",
    "        init.xavier_normal_(self.weight)\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(1, 1, out_c))\n",
    "            init.zeros_(self.bias)\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "        self.K = K + 1\n",
    "    def forward(self, inputs, graph):\n",
    "        \"\"\"\n",
    "        :param inputs: he input data, [B, N, C]\n",
    "        :param graph: the graph structure, [N, N]\n",
    "        :return: convolution result, [B, N, D]\n",
    "        \"\"\"\n",
    "        L = ChebConv.get_laplacian(graph, self.normalize)  # [N, N]\n",
    "        mul_L = self.cheb_polynomial(L).unsqueeze(1)  # [K, 1, N, N]\n",
    "        result = torch.matmul(mul_L, inputs)  # [K, B, N, C]\n",
    "        result = torch.matmul(result, self.weight)  # [K, B, N, D]\n",
    "        result = torch.sum(result, dim=0) + self.bias  # [B, N, D]\n",
    "        return result\n",
    "    def cheb_polynomial(self, laplacian):\n",
    "        \"\"\"\n",
    "        Compute the Chebyshev Polynomial, according to the graph laplacian\n",
    "\n",
    "        :param laplacian: the multi order Chebyshev laplacian, [K, N, N]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        N = laplacian.size(0)  # [N, N]\n",
    "        multi_order_laplacian = torch.zeros([self.K, N, N], device=laplacian.device, dtype=torch.float)  # [K, N, N]\n",
    "        multi_order_laplacian[0] = torch.eye(N, device=laplacian.device, dtype=torch.float)\n",
    "        if self.K == 1:\n",
    "            return multi_order_laplacian\n",
    "        else:\n",
    "            multi_order_laplacian[1] = laplacian\n",
    "            if self.K == 2:\n",
    "                return multi_order_laplacian\n",
    "            else:\n",
    "                for k in range(2, self.K):\n",
    "                    multi_order_laplacian[k] = 2 * torch.mm(laplacian, multi_order_laplacian[k - 1]) - \\\n",
    "                                               multi_order_laplacian[k - 2]\n",
    "        return multi_order_laplacian\n",
    "    @staticmethod\n",
    "    def get_laplacian(graph, normalize):\n",
    "        \"\"\"\n",
    "        compute the laplacian of the graph\n",
    "        :param graph: the graph structure without self loop, [N, N]\n",
    "        :param normalize: whether to used the normalized laplacian\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if normalize:\n",
    "            D = torch.diag(torch.sum(graph, dim=-1) ** (-1 / 2))\n",
    "            L = torch.eye(graph.size(0), device=graph.device, dtype=graph.dtype) - torch.mm(torch.mm(D, graph), D)\n",
    "        else:\n",
    "            D = torch.diag(torch.sum(graph, dim=-1))\n",
    "            L = D - graph\n",
    "        return L\n",
    "class ChebNet(nn.Module):\n",
    "    def __init__(self, in_c, hid_c, out_c, K):\n",
    "        \"\"\"\n",
    "        :param in_c: int, number of input channels.\n",
    "        :param hid_c: int, number of hidden channels.\n",
    "        :param out_c: int, number of output channels.\n",
    "        :param K:\n",
    "        \"\"\"\n",
    "        super(ChebNet, self).__init__()\n",
    "        self.conv1 = ChebConv(in_c=in_c, out_c=hid_c, K=K)\n",
    "        self.conv2 = ChebConv(in_c=hid_c, out_c=out_c, K=K)\n",
    "        self.act = nn.ReLU()\n",
    "    def forward(self, data):\n",
    "        graph_data = data[\"graph\"][0]  # [N, N]\n",
    "        flow_x = data[\"flow_x\"]  # [B, N, H, D]\n",
    "        B, N = flow_x.size(0), flow_x.size(1)\n",
    "        flow_x = flow_x.view(B, N, -1)  # [B, N, H*D]\n",
    "        output_1 = self.act(self.conv1(flow_x, graph_data))\n",
    "        output_2 = self.act(self.conv2(output_1, graph_data))\n",
    "        return output_2.unsqueeze(2)\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_c, out_c, alpha=0.2):\n",
    "        \"\"\"\n",
    "        graph attention layer\n",
    "        :param in_c:\n",
    "        :param out_c:\n",
    "        :param alpha:\n",
    "        \"\"\"\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.in_c = in_c\n",
    "        self.out_c = out_c\n",
    "        self.alpha = alpha\n",
    "        self.W = nn.Parameter(torch.empty(size=(in_c, out_c)))\n",
    "        nn.init.xavier_normal_(self.W.data)\n",
    "        self.a = nn.Parameter(torch.empty(size=(2 * out_c, 1)))\n",
    "        nn.init.xavier_normal_(self.a.data)\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "    def forward(self, features, adj):\n",
    "        B, N = features.size(0), features.size(1)\n",
    "        adj = adj + torch.eye(N, dtype=adj.dtype).cuda()  # A+I\n",
    "        h = torch.matmul(features, self.W)  # [B,N,out_features]\n",
    "        # [B, N, N, 2 * out_features]\n",
    "        a_input = torch.cat([h.repeat(1, 1, N).view(B, N * N, -1), h.repeat(1, N, 1)], dim=2).view(B, N, -1, 2 * self.out_c)\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(3))  # [B,N, N, 1] => [B, N, N]\n",
    "        zero_vec = -1e12 * torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)  # [B,N,N]\n",
    "        attention = F.softmax(attention, dim=2)  # softmax [N, N]\n",
    "        h_prime = torch.matmul(attention, h)  # [B,N, N]*[N, out_features] => [B,N, out_features]\n",
    "        return h_prime\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_c, hid_c, out_c, n_heads=6):\n",
    "        \"\"\"\n",
    "        :param in_c: int, number of input channels.\n",
    "        :param hid_c: int, number of hidden channels.\n",
    "        :param out_c: int, number of output channels.\n",
    "        :param n_heads: how many heads\n",
    "        \"\"\"\n",
    "        super(GAT, self).__init__()\n",
    "        self.attentions = nn.ModuleList([GraphAttentionLayer(in_c, hid_c) for _ in range(n_heads)])\n",
    "        self.conv2 = GraphAttentionLayer(hid_c*n_heads, out_c)\n",
    "        self.act = nn.ELU()\n",
    "    def forward(self, data):\n",
    "        # data prepare\n",
    "        adj = data[\"graph\"][0]  # [N, N]\n",
    "        x = data[\"flow_x\"]  # [B, N, H, D]\n",
    "        B, N = x.size(0), x.size(1)\n",
    "        x = x.view(B, N, -1)  # [B, N, H*D]\n",
    "        # forward\n",
    "        outputs = self.act(torch.cat([attention(x, adj) for attention in self.attentions], dim=-1))\n",
    "        output_2 = self.act(self.conv2(outputs, adj))\n",
    "        return output_2.unsqueeze(2)  # [B,1,N,1]\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        x=data[\"flow_x\"]\n",
    "        B,N=x.size(0), x.size(1)\n",
    "        x = x.view(B, N, -1) \n",
    "        return self.linear(x)\n",
    "        \n",
    "def MAE(y_true, y_pre):\n",
    "    y_true = (y_true).detach().numpy().copy().reshape((-1, 1))\n",
    "    y_pre = (y_pre).detach().numpy().copy().reshape((-1, 1))\n",
    "    re = np.abs(y_true - y_pre).mean()\n",
    "    return re\n",
    "def RMSE(y_true, y_pre):\n",
    "    y_true = (y_true).detach().numpy().copy().reshape((-1, 1))\n",
    "    y_pre = (y_pre).detach().numpy().copy().reshape((-1, 1))\n",
    "    re = math.sqrt(((y_true - y_pre) ** 2).mean())\n",
    "    return re\n",
    "def MAPE(y_true, y_pre):\n",
    "    y_true = (y_true).detach().numpy().copy().reshape((-1, 1))\n",
    "    y_pre = (y_pre).detach().numpy().copy().reshape((-1, 1))\n",
    "    e = (y_true + y_pre) / 2 + 1e-2\n",
    "    re = (np.abs(y_true - y_pre) / (np.abs(y_true) + e)).mean()\n",
    "    return re\n",
    "def R2(y_true, y_pre):\n",
    "    y_true = (y_true).detach().numpy().copy().reshape((-1, 1))\n",
    "    y_pre = (y_pre).detach().numpy().copy().reshape((-1, 1))\n",
    "    re = (np.abs(r2_score(y_true,y_pre))).mean()\n",
    "    return re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjacent_matrix(distance_file: str, num_nodes: int, id_file: str = None, graph_type=\"connect\") -> np.array:\n",
    "    \"\"\"\n",
    "    construct adjacent matrix by csv file\n",
    "    :param distance_file: path of csv file to save the distances between nodes\n",
    "    :param num_nodes: number of nodes in the graph\n",
    "    :param id_file: path of txt file to save the order of the nodes\n",
    "    :param graph_type: [\"connect\", \"distance\"] if use weight, please set distance\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    A = np.zeros([int(num_nodes), int(num_nodes)])\n",
    "    if id_file:\n",
    "        with open(id_file, \"r\") as f_id:\n",
    "            node_id_dict = {int(node_id): idx for idx, node_id in enumerate(f_id.read().strip().split(\"\\n\"))}\n",
    "\n",
    "            with open(distance_file, \"r\") as f_d:\n",
    "                f_d.readline()\n",
    "                reader = csv.reader(f_d)\n",
    "                for item in reader:\n",
    "                    if len(item) != 3:\n",
    "                        continue\n",
    "                    i, j, distance = int(item[0]), int(item[1]), float(item[2])\n",
    "                    if graph_type == \"connect\":\n",
    "                        A[node_id_dict[i], node_id_dict[j]] = 1.\n",
    "                        A[node_id_dict[j], node_id_dict[i]] = 1.\n",
    "                    elif graph_type == \"distance\":\n",
    "                        A[node_id_dict[i], node_id_dict[j]] = 1. / distance\n",
    "                        A[node_id_dict[j], node_id_dict[i]] = 1. / distance\n",
    "                    else:\n",
    "                        raise ValueError(\"graph type is not correct (connect or distance)\")\n",
    "        return A\n",
    "    with open(distance_file, \"r\") as f_d:\n",
    "        f_d.readline()\n",
    "        reader = csv.reader(f_d)\n",
    "        for item in reader:\n",
    "            if len(item) != 3:\n",
    "                continue\n",
    "            i, j, distance = int(item[0]), int(item[1]), float(item[2])\n",
    "\n",
    "            if graph_type == \"connect\":\n",
    "                A[i, j], A[j, i] = 1., 1.\n",
    "            elif graph_type == \"distance\":\n",
    "                A[i, j] = 1. / distance\n",
    "                A[j, i] = 1. / distance\n",
    "            else:\n",
    "                raise ValueError(\"graph type is not correct (connect or distance)\")\n",
    "    return A\n",
    "def get_flow_data(flow_file: str) -> np.array:\n",
    "    \"\"\"\n",
    "    parse npz to get flow data\n",
    "    :param flow_file: (N, T, D)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data = np.load(flow_file)\n",
    "    flow_data = data['data'].transpose([1, 0, 2])[:, :, 0][:, :, np.newaxis]  # [N, T, D]  D = 1\n",
    "    return flow_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PEMSDataset(Dataset):\n",
    "    def __init__(self, data_path, num_nodes, divide_days, time_interval, history_length, train_mode):\n",
    "        \"\"\"\n",
    "        load processed data\n",
    "        :param data_path: [\"graph file name\" , \"flow data file name\"], path to save the data file names\n",
    "        :param num_nodes: number of nodes in graph\n",
    "        :param divide_days: [ days of train data, days of test data], list to divide the original data\n",
    "        :param time_interval: time interval between two traffic data records (mins)\n",
    "        :param history_length: length of history data to be used\n",
    "        :param train_mode: [\"train\", \"test\"]\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.num_nodes = num_nodes\n",
    "        self.train_mode = train_mode\n",
    "        self.train_days = divide_days[0]\n",
    "        self.test_days = divide_days[1]\n",
    "        self.history_length = history_length  # 6\n",
    "        self.time_interval = time_interval  # 5 min\n",
    "        self.one_day_length = int(80/ self.time_interval)\n",
    "        self.graph = get_adjacent_matrix(distance_file=data_path[0], num_nodes=num_nodes)\n",
    "        self.flow_norm, self.flow_data = self.pre_process_data(data=get_flow_data(data_path[1]), norm_dim=1)\n",
    "    def __len__(self):\n",
    "        if self.train_mode == \"train\":\n",
    "            return self.train_days * self.one_day_length - self.history_length\n",
    "        elif self.train_mode == \"test\":\n",
    "            return self.test_days * self.one_day_length\n",
    "        else:\n",
    "            raise ValueError(\"train mode: [{}] is not defined\".format(self.train_mode))\n",
    "    def __getitem__(self, index):  # (x, y), index = [0, L1 - 1]\n",
    "        if self.train_mode == \"train\":\n",
    "            index = index\n",
    "        elif self.train_mode == \"test\":\n",
    "            index += self.train_days * self.one_day_length\n",
    "        else:\n",
    "            raise ValueError(\"train mode: [{}] is not defined\".format(self.train_mode))\n",
    "\n",
    "        data_x, data_y = PEMSDataset.slice_data(self.flow_data, self.history_length, index, self.train_mode)\n",
    "        data_x = PEMSDataset.to_tensor(data_x)  # [N, H, D]\n",
    "        data_y = PEMSDataset.to_tensor(data_y).unsqueeze(1)  # [N, 1, D]\n",
    "        return {\"graph\": PEMSDataset.to_tensor(self.graph), \"flow_x\": data_x, \"flow_y\": data_y}\n",
    "    @staticmethod\n",
    "    def slice_data(data, history_length, index, train_mode):\n",
    "        \"\"\"\n",
    "        :param data: np.array, normalized traffic data.\n",
    "        :param history_length: int, length of history data to be used.\n",
    "        :param index: int, index on temporal axis.\n",
    "        :param train_mode: str, [\"train\", \"test\"].\n",
    "        :return:\n",
    "            data_x: np.array, [N, H, D].\n",
    "            data_y: np.array [N, D].\n",
    "        \"\"\"\n",
    "        if train_mode == \"train\":\n",
    "            start_index = index\n",
    "            end_index = index + history_length\n",
    "        elif train_mode == \"test\":\n",
    "            start_index = index - history_length\n",
    "            end_index = index\n",
    "        else:\n",
    "            raise ValueError(\"train model {} is not defined\".format(train_mode))\n",
    "        data_x = data[:, start_index: end_index]\n",
    "        data_y = data[:, end_index]\n",
    "        return data_x, data_y\n",
    "    @staticmethod\n",
    "    def pre_process_data(data, norm_dim):\n",
    "        \"\"\"\n",
    "        :param data: np.array, original traffic data without normalization.\n",
    "        :param norm_dim: int, normalization dimension.\n",
    "        :return:\n",
    "            norm_base: list, [max_data, min_data], data of normalization base.\n",
    "            norm_data: np.array, normalized traffic data.\n",
    "        \"\"\"\n",
    "        norm_base = PEMSDataset.normalize_base(data, norm_dim)  # find the normalize base\n",
    "        norm_data = PEMSDataset.normalize_data(norm_base[0], norm_base[1], data)  # normalize data\n",
    "\n",
    "        return norm_base, norm_data\n",
    "    @staticmethod\n",
    "    def normalize_base(data, norm_dim):\n",
    "        \"\"\"\n",
    "        :param data: np.array, original traffic data without normalization.\n",
    "        :param norm_dim: int, normalization dimension.\n",
    "        :return:\n",
    "            max_data: np.array\n",
    "            min_data: np.array\n",
    "        \"\"\"\n",
    "        max_data = np.max(data, norm_dim, keepdims=True)  # [N, T, D] , norm_dim=1, [N, 1, D]\n",
    "        min_data = np.min(data, norm_dim, keepdims=True)\n",
    "        return max_data, min_data\n",
    "    @staticmethod\n",
    "    def normalize_data(max_data, min_data, data):\n",
    "        \"\"\"\n",
    "        :param max_data: np.array, max data.\n",
    "        :param min_data: np.array, min data.\n",
    "        :param data: np.array, original traffic data without normalization.\n",
    "        :return:\n",
    "            np.array, normalized traffic data.\n",
    "        \"\"\"\n",
    "        mid = min_data\n",
    "        base = max_data - min_data\n",
    "        normalized_data = (data - mid) / base\n",
    "\n",
    "        return normalized_data\n",
    "    @staticmethod\n",
    "    def recover_data(max_data, min_data, data):\n",
    "        \"\"\"\n",
    "        :param max_data: np.array, max data.\n",
    "        :param min_data: np.array, min data.\n",
    "        :param data: np.array, normalized data.\n",
    "        :return:\n",
    "            recovered_data: np.array, recovered data.\n",
    "        \"\"\"\n",
    "        mid = min_data\n",
    "        base = max_data - min_data\n",
    "        recovered_data = data * base + mid\n",
    "        return recovered_data\n",
    "    @staticmethod\n",
    "    def to_tensor(data):\n",
    "        return torch.tensor(data, dtype=torch.float)\n",
    "def get_loader(ds_name=\"PEMS04\"):\n",
    "    num_nodes = 1 if ds_name == 'PEMS04' else 1\n",
    "    train_data = PEMSDataset(data_path=[distance_path, \n",
    "                                        data_path_npz_2], num_nodes=num_nodes,\n",
    "                             divide_days=[25,15],\n",
    "                             time_interval=2, history_length=6,\n",
    "                             train_mode=\"train\")\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "    test_data = PEMSDataset(data_path=[distance_path, \n",
    "                                        data_path_npz_2], num_nodes=num_nodes,\n",
    "                            divide_days=[25,15],\n",
    "                            time_interval=2, history_length=6,\n",
    "                            train_mode=\"test\")\n",
    "    test_loader = DataLoader(test_data, batch_size=12, shuffle=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2020\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "plt.rcParams['font.sans-serif'] = ['simhei']  # 用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号\n",
    "# data\n",
    "train_loader, test_loader = get_loader('PEMS04')\n",
    "gcn = GCN(6, 3, 1)\n",
    "chebnet = ChebNet(6, 3, 1, 1)\n",
    "gat = GAT(6, 3, 1)\n",
    "mlp=MLP(6,1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "models = [\n",
    "    chebnet.to(device),\n",
    "    gat.to(device),\n",
    "    gcn.to(device),\n",
    "    #mlp.to(device)\n",
    "]\n",
    "all_predict_values = []\n",
    "epochs = 50\n",
    "for i in range(len(models)):\n",
    "    model = models[i]\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    optimizer = optim.Adam(params=model.parameters(), lr=3e-2)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss, epoch_mae, epoch_rmse, epoch_mape = 0.0, 0.0, 0.0, 0.0\n",
    "        num = 0\n",
    "        start_time = time.time()\n",
    "        for data in train_loader:  # [\"graph\": [B, N, N] , \"flow_x\": [B, N, H, D], \"flow_y\": [B, N, 1, D]]\n",
    "            data['graph'], data['flow_x'], data['flow_y'] = data['graph'].to(device), data['flow_x'].to(device), data['flow_y'].to(device)\n",
    "            predict_value = model(data)  # [0, 1] -> recover\n",
    "            loss = criterion(predict_value, data[\"flow_y\"])\n",
    "            epoch_mae += MAE(data[\"flow_y\"].cpu(), predict_value.cpu())\n",
    "            epoch_rmse += RMSE(data[\"flow_y\"].cpu(), predict_value.cpu())\n",
    "            epoch_mape += MAPE(data[\"flow_y\"].cpu(), predict_value.cpu())\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            num += 1\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        end_time = time.time()\n",
    "        epoch_mae = epoch_mae / num\n",
    "        epoch_rmse = epoch_rmse / num\n",
    "        epoch_mape = epoch_mape / num\n",
    "        print(\n",
    "            \"Epoch: {:04d}, Loss: {:02.4f}, mae: {:02.4f}, rmse: {:02.4f}, mape: {:02.4f}, Time: {:02.2f} mins\".format(\n",
    "                epoch + 1, 10 * epoch_loss / (len(train_loader.dataset) / 64),\n",
    "                epoch_mae, epoch_rmse, epoch_mape, (end_time - start_time) ))\n",
    "    \n",
    "        model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.0\n",
    "        num = 0\n",
    "        all_predict_value = 0\n",
    "        all_y_true = 0\n",
    "        for data in test_loader:\n",
    "            data['graph'], data['flow_x'], data['flow_y'] = data['graph'].to(device), data['flow_x'].to(device), data['flow_y'].to(device)\n",
    "            predict_value = model(data)\n",
    "            if num == 0:\n",
    "                all_predict_value = predict_value\n",
    "                all_y_true = data[\"flow_y\"]\n",
    "            else:\n",
    "                all_predict_value = torch.cat([all_predict_value, predict_value], dim=0)\n",
    "                all_y_true = torch.cat([all_y_true, data[\"flow_y\"]], dim=0)\n",
    "            loss = criterion(predict_value, data[\"flow_y\"])\n",
    "            total_loss += loss.item()\n",
    "            num += 1\n",
    "        epoch_mae = MAE(all_y_true.cpu(), all_predict_value.cpu())\n",
    "        epoch_rmse = RMSE(all_y_true.cpu(), all_predict_value.cpu())\n",
    "        epoch_mape = MAPE(all_y_true.cpu(), all_predict_value.cpu())\n",
    "        print(\"Test Loss: {:02.4f}, mae: {:02.4f}, rmse: {:02.4f}, mape: {:02.4f}\".format(\n",
    "            10 * total_loss / (len(test_loader.dataset) / 64), epoch_mae, epoch_rmse, epoch_mape))\n",
    "    all_predict_values.append(all_predict_value.cpu())\n",
    "    size = sum([param.nelement() for param in model.parameters()])\n",
    "    print('size=',size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
